# pip install pymilvus langchain-milvus
# Milvusæ–‡æ¡£ https://milvus.io/docs/zh
# å½“å‰ä»£ç ä½¿ç”¨ Milvus Distributedéƒ¨ç½²ç‰ˆæœ¬
import os
from pymilvus import (
    connections,
    FieldSchema, CollectionSchema, DataType,
    Collection, utility
)
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_milvus import Milvus
import numpy as np
from typing import List, Dict, Any, Optional

class MilvusVectorManager:
	def __init__(self,host:str="localhost",port:str="19530",collection_name:str="document_vectors"):
		self.host=host
		self.port=port
		self.collection_name=collection_name
		self.collection=None
		self.embedding_function=None
	def connect(self):
		try:
			connections.connect(alias="default",host=self.host,port=self.port)
			print(f"è¿æ¥æˆåŠŸ Milvus:{self.host}:{self.port}")
			return True
		except:
			return False
	def initialize_embedding_function(self,model_name:str="BAAI/bge-large-zh-v1.5",device:str="mps",normalize_embeddings:bool=True):
		model_kwargs={"device":device}
		encode_kwargs={'normalize_embeddings': normalize_embeddings}

		self.embedding_function=HuggingFaceEmbeddings(
			model_name=model_name,
			model_kwargs=model_kwargs,
			encode_kwargs=encode_kwargs
		)
        
        # æµ‹è¯•åµŒå…¥ç»´åº¦
        test_embedding = self.embedding_function.embed_documents(["test"])
        self.dimension = len(test_embedding[0])
        print(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œç»´åº¦: {self.dimension}")
        
        return self.dimension
    
    def create_collection(self, overwrite: bool = False):
        """åˆ›å»ºé›†åˆ"""
        if not self.connect():
            return False
            
        # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
        if utility.has_collection(self.collection_name):
            if overwrite:
                utility.drop_collection(self.collection_name)
                print(f"å·²åˆ é™¤ç°æœ‰é›†åˆ: {self.collection_name}")
            else:
                print(f"é›†åˆå·²å­˜åœ¨: {self.collection_name}")
                self.collection = Collection(self.collection_name)
                return True
        
        # å®šä¹‰å­—æ®µ
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.dimension),
            FieldSchema(name="metadata", dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=255),
            FieldSchema(name="created_time", dtype=DataType.INT64),
        ]
        
        # åˆ›å»ºé›†åˆæ¶æ„
        schema = CollectionSchema(fields, description="æ–‡æ¡£å‘é‡å­˜å‚¨")
        
        # åˆ›å»ºé›†åˆ
        self.collection = Collection(name=self.collection_name, schema=schema)
        
        # åˆ›å»ºç´¢å¼•
        index_params = {
            "index_type": "IVF_FLAT",
            "metric_type": "L2",
            "params": {"nlist": 128}
        }
        
        self.collection.create_index("embedding", index_params)
        print(f"åˆ›å»ºé›†åˆæˆåŠŸ: {self.collection_name}")
        return True
    
    def insert_documents(self, documents: List[Any], batch_size: int = 100):
        """æ’å…¥æ–‡æ¡£"""
        if not self.collection:
            print("è¯·å…ˆåˆ›å»ºé›†åˆ")
            return False
        
        total_docs = len(documents)
        inserted_count = 0
        
        for i in range(0, total_docs, batch_size):
            batch_docs = documents[i:i + batch_size]
            texts = [doc.page_content for doc in batch_docs]
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embeddings = self.embedding_function.embed_documents(texts)
            
            # å‡†å¤‡æ’å…¥æ•°æ®
            entities = []
            for j, doc in enumerate(batch_docs):
                entities.append([
                    doc.page_content,  # text
                    embeddings[j],     # embedding
                    str(doc.metadata), # metadata
                    doc.metadata.get('source', 'unknown'),  # source
                    int(os.times().elapsed)  # created_time
                ])
            
            # æ’å…¥æ•°æ®
            insert_result = self.collection.insert(entities)
            inserted_count += len(batch_docs)
            print(f"å·²æ’å…¥ {inserted_count}/{total_docs} ä¸ªæ–‡æ¡£")
        
        # åˆ·æ–°æ•°æ®
        self.collection.flush()
        print(f"æˆåŠŸæ’å…¥ {inserted_count} ä¸ªæ–‡æ¡£")
        return inserted_count
    
    def search_similar(self, 
                      query: str, 
                      k: int = 5,
                      filters: Optional[Dict] = None) -> List[Dict]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        if not self.collection:
            print("è¯·å…ˆåˆ›å»ºé›†åˆ")
            return []
        
        # åŠ è½½é›†åˆåˆ°å†…å­˜
        self.collection.load()
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_function.embed_query(query)
        
        # æ„å»ºæœç´¢å‚æ•°
        search_params = {
            "metric_type": "L2",
            "params": {"nprobe": 10}
        }
        
        # æ„å»ºè¿‡æ»¤è¡¨è¾¾å¼
        expr = None
        if filters:
            filter_parts = []
            for key, value in filters.items():
                if isinstance(value, str):
                    filter_parts.append(f'{key} == "{value}"')
                else:
                    filter_parts.append(f'{key} == {value}')
            expr = " and ".join(filter_parts)
        
        # æ‰§è¡Œæœç´¢
        results = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param=search_params,
            limit=k,
            expr=expr,
            output_fields=["id", "text", "metadata", "source"]
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for hits in results:
            for hit in hits:
                formatted_results.append({
                    "id": hit.id,
                    "text": hit.entity.get("text"),
                    "metadata": eval(hit.entity.get("metadata", "{}")),
                    "source": hit.entity.get("source"),
                    "score": hit.score,
                    "distance": hit.distance
                })
        
        return formatted_results
    
    def update_document(self, doc_id: int, new_text: str, new_metadata: Dict = None):
        """æ›´æ–°æ–‡æ¡£"""
        if not self.collection:
            print("è¯·å…ˆåˆ›å»ºé›†åˆ")
            return False
        
        # ç”Ÿæˆæ–°çš„åµŒå…¥å‘é‡
        new_embedding = self.embedding_function.embed_documents([new_text])[0]
        
        # å‡†å¤‡æ›´æ–°æ•°æ®
        update_data = {
            "text": new_text,
            "embedding": new_embedding,
            "metadata": str(new_metadata) if new_metadata else "{}"
        }
        
        try:
            # Milvus 2.x ä½¿ç”¨ upsert è¿›è¡Œæ›´æ–°
            entities = [[
                new_text,
                new_embedding,
                str(new_metadata) if new_metadata else "{}",
                "updated",  # source
                int(os.times().elapsed)  # created_time
            ]]
            
            # å…ˆåˆ é™¤æ—§æ–‡æ¡£ï¼Œå†æ’å…¥æ–°æ–‡æ¡£ï¼ˆæ¨¡æ‹Ÿæ›´æ–°ï¼‰
            self.delete_documents([doc_id])
            self.collection.insert(entities)
            self.collection.flush()
            
            print(f"æˆåŠŸæ›´æ–°æ–‡æ¡£ ID: {doc_id}")
            return True
            
        except Exception as e:
            print(f"æ›´æ–°æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def delete_documents(self, doc_ids: List[int]):
        """åˆ é™¤æ–‡æ¡£"""
        if not self.collection:
            print("è¯·å…ˆåˆ›å»ºé›†åˆ")
            return False
        
        try:
            # æ„å»ºåˆ é™¤è¡¨è¾¾å¼
            ids_str = ", ".join(map(str, doc_ids))
            expr = f"id in [{ids_str}]"
            
            # æ‰§è¡Œåˆ é™¤
            result = self.collection.delete(expr)
            self.collection.flush()
            
            print(f"æˆåŠŸåˆ é™¤ {len(doc_ids)} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            print(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def get_collection_stats(self):
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        if not self.collection:
            print("è¯·å…ˆåˆ›å»ºé›†åˆ")
            return None
        
        stats = {
            "collection_name": self.collection_name,
            "num_entities": self.collection.num_entities,
            "is_empty": self.collection.is_empty
        }
        
        print(f"é›†åˆç»Ÿè®¡:")
        print(f"  - åç§°: {stats['collection_name']}")
        print(f"  - æ–‡æ¡£æ•°é‡: {stats['num_entities']}")
        print(f"  - æ˜¯å¦ä¸ºç©º: {stats['is_empty']}")
        
        return stats
    
    def list_collections(self):
        """åˆ—å‡ºæ‰€æœ‰é›†åˆ"""
        if not self.connect():
            return []
        
        collections = utility.list_collections()
        print("æ‰€æœ‰é›†åˆ:")
        for col in collections:
            print(f"  - {col}")
        
        return collections
    
    def drop_collection(self):
        """åˆ é™¤é›†åˆ"""
        if not self.connect():
            return False
        
        if utility.has_collection(self.collection_name):
            utility.drop_collection(self.collection_name)
            print(f"æˆåŠŸåˆ é™¤é›†åˆ: {self.collection_name}")
            return True
        else:
            print(f"é›†åˆä¸å­˜åœ¨: {self.collection_name}")
            return False
    
    def backup_collection(self, backup_path: str):
        """å¤‡ä»½é›†åˆæ•°æ®"""
        if not self.collection:
            print("è¯·å…ˆåˆ›å»ºé›†åˆ")
            return False
        
        try:
            # æŸ¥è¯¢æ‰€æœ‰æ•°æ®
            self.collection.load()
            results = self.collection.query(
                expr="id >= 0",
                output_fields=["id", "text", "metadata", "source"]
            )
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            import json
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"å¤‡ä»½æˆåŠŸï¼Œä¿å­˜åˆ°: {backup_path}")
            print(f"å¤‡ä»½äº† {len(results)} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            print(f"å¤‡ä»½å¤±è´¥: {e}")
            return False

# ä½¿ç”¨ LangChain çš„ Milvus é›†æˆ
class LangChainMilvusManager:
    def __init__(self, 
                 connection_args: Dict = None,
                 collection_name: str = "langchain_docs"):
        self.connection_args = connection_args or {
            "host": "localhost",
            "port": "19530"
        }
        self.collection_name = collection_name
        self.vectorstore = None
        
    def initialize(self, embedding_function, documents: List[Any] = None):
        """åˆå§‹åŒ– LangChain Milvus"""
        if documents:
            # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
            self.vectorstore = Milvus.from_documents(
                documents=documents,
                embedding=embedding_function,
                collection_name=self.collection_name,
                connection_args=self.connection_args
            )
            print(f"åˆ›å»ºæ–°çš„ Milvus å‘é‡å­˜å‚¨ï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£")
        else:
            # åŠ è½½ç°æœ‰å‘é‡å­˜å‚¨
            self.vectorstore = Milvus(
                embedding_function=embedding_function,
                collection_name=self.collection_name,
                connection_args=self.connection_args
            )
            print("åŠ è½½ç°æœ‰ Milvus å‘é‡å­˜å‚¨")
        
        return self.vectorstore
    
    def add_documents(self, documents: List[Any]):
        """æ·»åŠ æ–‡æ¡£"""
        if not self.vectorstore:
            print("è¯·å…ˆåˆå§‹åŒ–å‘é‡å­˜å‚¨")
            return False
        
        self.vectorstore.add_documents(documents)
        print(f"æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£")
        return True
    
    def similarity_search(self, query: str, k: int = 5, **kwargs):
        """ç›¸ä¼¼åº¦æœç´¢"""
        if not self.vectorstore:
            print("è¯·å…ˆåˆå§‹åŒ–å‘é‡å­˜å‚¨")
            return []
        
        results = self.vectorstore.similarity_search(query, k=k, **kwargs)
        return results
    
    def delete_documents(self, ids: List[str]):
        """åˆ é™¤æ–‡æ¡£ï¼ˆé€šè¿‡ IDï¼‰"""
        # æ³¨æ„ï¼šLangChain Milvus çš„åˆ é™¤åŠŸèƒ½å¯èƒ½æœ‰é™
        # é€šå¸¸éœ€è¦ç›´æ¥ä½¿ç”¨ pymilvus
        print("LangChain Milvus åˆ é™¤åŠŸèƒ½æœ‰é™ï¼Œå»ºè®®ä½¿ç”¨åŸç”Ÿ pymilvus")
        return False


def main():
    """Milvus å®Œæ•´åŠŸèƒ½æ¼”ç¤º"""
    
    # 1. åˆå§‹åŒ–ç®¡ç†å™¨
    print("=== 1. åˆå§‹åŒ– Milvus ç®¡ç†å™¨ ===")
    manager = MilvusVectorManager(
        host="localhost",  # æ ¹æ®ä½ çš„ Milvus é…ç½®ä¿®æ”¹
        port="19530",
        collection_name="three_kingdoms"
    )
    
    # 2. åˆå§‹åŒ–åµŒå…¥å‡½æ•°
    print("\n=== 2. åˆå§‹åŒ–åµŒå…¥å‡½æ•° ===")
    dimension = manager.initialize_embedding_function(
        model_name="BAAI/bge-large-zh-v1.5",
        device="cpu",  # å¦‚æœæœ‰ GPU å¯ä»¥æ”¹ä¸º "cuda"
        normalize_embeddings=True
    )
    
    # 3. åˆ›å»ºé›†åˆ
    print("\n=== 3. åˆ›å»ºé›†åˆ ===")
    manager.create_collection(overwrite=True)  # è¦†ç›–ç°æœ‰é›†åˆ
    
    # 4. åŠ è½½å’Œå¤„ç†æ–‡æ¡£
    print("\n=== 4. åŠ è½½å’Œå¤„ç†æ–‡æ¡£ ===")
    try:
        documents = get_documents('../Embedding/source/three_kingdoms.txt')
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, 
            chunk_overlap=100
        )
        split_docs = text_splitter.split_documents(documents)
        print(f"åŠ è½½äº† {len(split_docs)} ä¸ªæ–‡æ¡£å—")
    except Exception as e:
        print(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {e}")
        # ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£
        from langchain_core.documents import Document
        split_docs = [
            Document(page_content="åˆ˜å¤‡å­—ç„å¾·ï¼Œä¸­å±±é–ç‹åˆ˜èƒœä¹‹å", metadata={"source": "sample"}),
            Document(page_content="å…³ç¾½å­—äº‘é•¿ï¼Œæ²³ä¸œè§£è‰¯äººä¹Ÿ", metadata={"source": "sample"}),
            Document(page_content="å¼ é£å­—ç¿¼å¾·ï¼Œæ¶¿éƒ¡äººä¹Ÿ", metadata={"source": "sample"})
        ]
        print(f"ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£: {len(split_docs)} ä¸ª")
    
    # 5. æ’å…¥æ–‡æ¡£
    print("\n=== 5. æ’å…¥æ–‡æ¡£ ===")
    inserted_count = manager.insert_documents(split_docs, batch_size=50)
    
    # 6. è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\n=== 6. é›†åˆç»Ÿè®¡ ===")
    stats = manager.get_collection_stats()
    
    # 7. æœç´¢æ¼”ç¤º
    print("\n=== 7. æœç´¢æ¼”ç¤º ===")
    test_queries = ["åˆ˜å¤‡", "å…³ç¾½", "æ›¹æ“", "è¯¸è‘›äº®"]
    
    for query in test_queries:
        print(f"\nğŸ” æœç´¢: '{query}'")
        results = manager.search_similar(query, k=3)
        
        for i, result in enumerate(results):
            print(f"  {i+1}. ID: {result['id']}, åˆ†æ•°: {result['score']:.4f}")
            print(f"     å†…å®¹: {result['text'][:80]}...")
            print(f"     æ¥æº: {result['source']}")
    
    # 8. æ›´æ–°æ–‡æ¡£æ¼”ç¤º
    print("\n=== 8. æ›´æ–°æ–‡æ¡£æ¼”ç¤º ===")
    if results:
        first_doc_id = results[0]['id']
        print(f"ğŸ”„ æ›´æ–°æ–‡æ¡£ ID: {first_doc_id}")
        manager.update_document(
            first_doc_id, 
            "åˆ˜å¤‡å­—ç„å¾·ï¼Œä¸­å±±é–ç‹åˆ˜èƒœä¹‹åï¼Œæ±‰æ™¯å¸é˜ä¸‹ç„å­™",  # æ–°å†…å®¹
            {"source": "updated", "version": "2.0"}
        )
    
    # 9. åˆ é™¤æ–‡æ¡£æ¼”ç¤º
    print("\n=== 9. åˆ é™¤æ–‡æ¡£æ¼”ç¤º ===")
    if len(results) >= 2:
        delete_ids = [results[1]['id']]
        print(f"ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£ ID: {delete_ids}")
        manager.delete_documents(delete_ids)
    
    # 10. æœ€ç»ˆç»Ÿè®¡
    print("\n=== 10. æœ€ç»ˆç»Ÿè®¡ ===")
    manager.get_collection_stats()
    
    # 11. å¤‡ä»½æ¼”ç¤º
    print("\n=== 11. å¤‡ä»½æ•°æ® ===")
    manager.backup_collection("./milvus_backup.json")
    
    # 12. ä½¿ç”¨ LangChain é›†æˆç‰ˆæœ¬
    print("\n=== 12. LangChain Milvus é›†æˆæ¼”ç¤º ===")
    lc_manager = LangChainMilvusManager(collection_name="langchain_docs")
    lc_vectorstore = lc_manager.initialize(manager.embedding_function, split_docs[:10])
    
    # LangChain æœç´¢
    lc_results = lc_manager.similarity_search("ä¸‰å›½è‹±é›„", k=2)
    for i, doc in enumerate(lc_results):
        print(f"  {i+1}. {doc.page_content[:80]}...")
        print(f"     å…ƒæ•°æ®: {doc.metadata}")

if __name__ == "__main__":
    main()
